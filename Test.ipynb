{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECODE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        from langchain_community.document_loaders import PDFPlumberLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_or_fetch_embeddings(index_name, chunks):\n",
    "#     # importing the necessary libraries and initializing the Pinecone client\n",
    "#     import pinecone\n",
    "#     from langchain_community.vectorstores import Pinecone\n",
    "#     from langchain_openai import OpenAIEmbeddings\n",
    "#     from pinecone import ServerlessSpec\n",
    "\n",
    "    \n",
    "#     pc = pinecone.Pinecone(pinecone_api_key)\n",
    "        \n",
    "#     embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "#     # loading from existing index\n",
    "#     if index_name in pc.list_indexes().names():\n",
    "#         print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "#         vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "#         print('Ok')\n",
    "#     else:\n",
    "#         # creating the index and embedding the chunks into the index \n",
    "#         print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "#         # creating a new index\n",
    "#         pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=1536,\n",
    "#             metric='cosine',\n",
    "#             spec=ServerlessSpec(\n",
    "#                 cloud=\"aws\",\n",
    "#                 region=\"us-east-1\"\n",
    "#         ) \n",
    "#         )\n",
    "\n",
    "#         # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "#         # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "#         vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "#         print('Ok')\n",
    "        \n",
    "#     return vector_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def delete_pinecone_index(index_name='all'):\n",
    "#     import pinecone\n",
    "#     pc = pinecone.Pinecone(pinecone_api_key)\n",
    "    \n",
    "#     if index_name == 'all':\n",
    "#         indexes = pc.list_indexes().names()\n",
    "#         print('Deleting all indexes ... ')\n",
    "#         for index in indexes:\n",
    "#             pc.delete_index(index)\n",
    "#         print('Ok')\n",
    "#     else:\n",
    "#         print(f'Deleting index {index_name} ...', end='')\n",
    "#         pc.delete_index(index_name)\n",
    "#         print('Ok')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=153)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document(\"rag_powered_by_google_search.pdf\")\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Using the documnet is this a good company to invest in?', 'result': \"I don't have access to specific documents or information about Google's financial performance or investment prospects. To determine if a company is good to invest in, it's recommended to conduct thorough research on their financial statements, competitive position, growth prospects, and other relevant factors. It's always advisable to consult with a financial advisor before making any investment decisions.\"}\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "# q = 'What was googles revenue in q1 of 2024?'\n",
    "q = 'Using the documnet is this a good company to invest in?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
    "\n",
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)  \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# def create_conversational_retrieval_chain(vector_store):\n",
    "#     # Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "#     llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "#     # Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "#     retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "\n",
    "#     # Create a memory buffer to track the conversation\n",
    "#     memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "#     # Create the Conversational Retrieval Chain\n",
    "#     crc = ConversationalRetrievalChain.from_llm(\n",
    "#         llm=llm,  # Link the ChatGPT LLM\n",
    "#         retriever=retriever,  # Link the vector store based retriever\n",
    "#         memory=memory,  # Link the conversation memory\n",
    "#         chain_type='stuff',  # Specify the chain type\n",
    "#         verbose=False  # Set to True to enable verbose logging for debugging\n",
    "#     )\n",
    "\n",
    "#     return crc\n",
    "\n",
    "# # Usage example:\n",
    "# # crc_instance = create_conversational_retrieval_chain(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def create_conversational_retrieval_chain(vector_store):\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "    # Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "    # Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "\n",
    "    # Create a memory buffer to track the conversation\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "    # Create the Conversational Retrieval Chain\n",
    "    crc = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,  # Link the ChatGPT LLM\n",
    "        retriever=retriever,  # Link the vector store based retriever\n",
    "        memory=memory,  # Link the conversation memory\n",
    "        chain_type='stuff',  # Specify the chain type\n",
    "        verbose=False  # Set to True to enable verbose logging for debugging\n",
    "    )\n",
    "\n",
    "    return crc\n",
    "\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result\n",
    "\n",
    "# Usage example:\n",
    "# Initialize the chain once\n",
    "# crc_instance = create_conversational_retrieval_chain(vector_store)\n",
    "\n",
    "# Ask questions with preserved memory\n",
    "# answer1 = ask_question(crc_instance, \"What is the capital of France?\")\n",
    "# print(answer1)\n",
    "\n",
    "# answer2 = ask_question(crc_instance, \"What is the population of that capital?\")\n",
    "# print(answer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document(\"rag_powered_by_google_search.pdf\")\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)\n",
    "crc_instance = create_conversational_retrieval_chain(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What was googles revenue in q1 of 2024?', 'chat_history': [HumanMessage(content='What was googles revenue in q1 of 2024?'), AIMessage(content=\"Google's revenue in Q1 of 2024 was $80,539 million.\")], 'answer': \"Google's revenue in Q1 of 2024 was $80,539 million.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'What was googles revenue in q1 of 2024?'\n",
    "result = ask_question(q, crc_instance)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What was googles revenue in q1 of 2024?',\n",
       " 'chat_history': [HumanMessage(content='What was googles revenue in q1 of 2024?'),\n",
       "  AIMessage(content=\"Google's revenue in Q1 of 2024 was $80,539 million.\")],\n",
       " 'answer': \"Google's revenue in Q1 of 2024 was $80,539 million.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Multiply that number by 2.', 'chat_history': [HumanMessage(content='What was googles revenue in q1 of 2024?'), AIMessage(content=\"Google's revenue in Q1 of 2024 was $80,539 million.\"), HumanMessage(content='Multiply that number by 2.'), AIMessage(content=\"Google's revenue in Q1 of 2024 was $69.787 billion. Multiplying this by 2 would result in $139.574 billion.\")], 'answer': \"Google's revenue in Q1 of 2024 was $69.787 billion. Multiplying this by 2 would result in $139.574 billion.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 2.'\n",
    "result = ask_question(q, crc_instance)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What was googles revenue in q1 of 2024?'\n",
      "content=\"Google's revenue in Q1 of 2024 was $80,539 million.\"\n",
      "content='Multiply the revenue number by 2.'\n",
      "content=\"Google's revenue in Q1 of 2024 was $69.787 billion. Multiplying this by 2 would result in $139.574 billion.\"\n",
      "content='What was googles revenue in q1 of 2024?'\n",
      "content=\"Google's revenue in Q1 of 2024 was $80,539 million. When multiplied by 2, the total revenue would be $161,078 million.\"\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document appears to be related to Alphabet Inc.'s Amended and Restated 2021 Stock Plan, specifically the Form of Alphabet Restricted Stock Unit Agreement.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The key information in this document includes details about Alphabet Inc.'s Amended and Restated 2021 Stock Plan, specifically the Form of Alphabet Restricted Stock Unit Agreement. Additionally, it mentions corporate governance information such as the certificate of incorporation, bylaws, and governance guidelines, which can be found on Google's Keyword blog at https://www.blog.google/.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Bye bye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, crc_instance)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "Before answering translate your response to Spanish.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt },\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
